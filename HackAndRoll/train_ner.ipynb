{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import warnings\n",
    "#import matplotlib.pyplot as plt\n",
    "# https://en.wikipedia.org/wiki/List_of_cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-worded food entities: 185\n",
      "2-worded food entities: 77\n",
      "3-worded food entities: 30\n"
     ]
    }
   ],
   "source": [
    "# read in the food csv file\n",
    "food_df = pd.read_csv(\"cuisines.csv\")\n",
    "food_df = food_df.sample(frac=1)\n",
    "\n",
    "# print row and column information\n",
    "food_df.head()\n",
    "\n",
    "# print the size \n",
    "food_df[\"food\"].size\n",
    "\n",
    "# diaqualify foods with special characters, lowercase and extract results from \"food\" column\n",
    "foods = food_df[food_df[\"food\"].str.contains(\"[^a-zA-Z ]\") == False][\"food\"].apply(lambda food: food.lower())\n",
    "\n",
    "# filter out foods with more than 3 words, drop any duplicates\n",
    "foods = foods[foods.str.split().apply(len) <= 3].drop_duplicates()\n",
    "# find one-worded, two-worded and three-worded foods\n",
    "one_worded_foods = foods[foods.str.split().apply(len) == 1]\n",
    "two_worded_foods = foods[foods.str.split().apply(len) == 2]\n",
    "three_worded_foods = foods[foods.str.split().apply(len) == 3]\n",
    "\n",
    "# total number of foods\n",
    "total_num_foods = round(one_worded_foods.size / 45 * 100)\n",
    "\n",
    "# shuffle the 2-worded and 3-worded foods since we'll be slicing them\n",
    "two_worded_foods = two_worded_foods.sample(frac=1)\n",
    "three_worded_foods = three_worded_foods.sample(frac=1)\n",
    "\n",
    "# append the foods together \n",
    "foods = one_worded_foods.append(two_worded_foods[:round(total_num_foods * 0.30)]).append(three_worded_foods[:round(total_num_foods * 0.25)])\n",
    "\n",
    "# print the resulting sizes\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}-worded food entities:\", foods[foods.str.split().apply(len) == i + 1].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_templates = [\n",
    "    \"I ate my {}\",\n",
    "    \"I'm eating a {}\",\n",
    "    \"I just ate a {}\",\n",
    "    \"I only ate the {}\",\n",
    "    \"I'm done eating a {}\",\n",
    "    \"I've already eaten a {}\",\n",
    "    \"I just finished my {}\",\n",
    "    \"When I was having lunch I ate a {}\",\n",
    "    \"I had a {} and a {} today\",\n",
    "    \"I ate a {} and a {} for lunch\",\n",
    "    \"I made a {} and {} for lunch\",\n",
    "    \"I ate {} and {}\",\n",
    "    \"today I ate a {} and a {} for lunch\",\n",
    "    \"I had {} with my husband last night\",\n",
    "    \"I brought you some {} on my birthday\",\n",
    "    \"I made {} for yesterday's dinner\",\n",
    "    \"last night, a {} was sent to me with {}\",\n",
    "    \"I had {} yesterday and I'd like to eat it anyway\",\n",
    "    \"I ate a couple of {} last night\",\n",
    "    \"I had some {} at dinner last night\",\n",
    "    \"Last night, I ordered some {}\",\n",
    "    \"I made a {} last night\",\n",
    "    \"I had a bowl of {} with {} and I wanted to go to the mall today\",\n",
    "    \"I brought a basket of {} for breakfast this morning\",\n",
    "    \"I had a bowl of {}\",\n",
    "    \"I ate a {} with {} in the morning\",\n",
    "    \"I made a bowl of {} for my breakfast\",\n",
    "    \"There's {} for breakfast in the bowl this morning\",\n",
    "    \"This morning, I made a bowl of {}\",\n",
    "    \"I decided to have some {} as a little bonus\",\n",
    "    \"I decided to enjoy some {}\",\n",
    "    \"I've decided to have some {} for dessert\",\n",
    "    \"I had a {}, a {} and {} at home\",\n",
    "    \"I took a {}, {} and {} on the weekend\",\n",
    "    \"I ate a {} with {} and {} just now\",\n",
    "    \"Last night, I ate an {} with {} and {}\",\n",
    "    \"I tasted some {}, {} and {} at the office\",\n",
    "    \"There's a basket of {}, {} and {} that I consumed\",\n",
    "    \"I devoured a {}, {} and {}\",\n",
    "    \"I've already had a bag of {}, {} and {} from the fridge\"\n",
    "]\n",
    "\n",
    "# create dictionaries to store the generated food combinations. Do note that one_food != one_worded_food. one_food == \"barbecue sauce\", one_worded_food == \"sauce\"\n",
    "TRAIN_FOOD_DATA = {\n",
    "    \"one_food\": [],\n",
    "    \"two_foods\": [],\n",
    "    \"three_foods\": []\n",
    "}\n",
    "\n",
    "TEST_FOOD_DATA = {\n",
    "    \"one_food\": [],\n",
    "    \"two_foods\": [],\n",
    "    \"three_foods\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_food, two_food, and three_food combinations will be limited to 167 sentences\n",
    "FOOD_SENTENCE_LIMIT = 80\n",
    "\n",
    "# helper function for deciding what dictionary and subsequent array to append the food sentence on to\n",
    "def get_food_data(count):\n",
    "    return {\n",
    "        1: TRAIN_FOOD_DATA[\"one_food\"] if len(TRAIN_FOOD_DATA[\"one_food\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"one_food\"],\n",
    "        2: TRAIN_FOOD_DATA[\"two_foods\"] if len(TRAIN_FOOD_DATA[\"two_foods\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"two_foods\"],\n",
    "        3: TRAIN_FOOD_DATA[\"three_foods\"] if len(TRAIN_FOOD_DATA[\"three_foods\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"three_foods\"],\n",
    "    }[count]\n",
    "\n",
    "# the pattern to replace from the template sentences\n",
    "pattern_to_replace = \"{}\"\n",
    "\n",
    "\n",
    "# the count that helps us decide when to break from the for loop\n",
    "food_entity_count = foods.size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 93,   6, 115, 334, 293,  20,  77,  17, 133, 328,\n",
       "            ...\n",
       "            201, 250, 230, 274, 286, 291, 265, 258, 228, 273],\n",
       "           dtype='int64', length=292)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_lst = foods.index\n",
    "idx_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the while loop, ensure we don't get an index out of bounds error\n",
    "while food_entity_count >= 2:\n",
    "    entities = []\n",
    "\n",
    "    # pick a random food template\n",
    "    sentence = food_templates[random.randint(0, len(food_templates) - 1)]\n",
    "\n",
    "    # find out how many braces \"{}\" need to be replaced in the template\n",
    "    matches = re.findall(pattern_to_replace, sentence)\n",
    "\n",
    "    # for each brace, replace with a food entity from the shuffled food data\n",
    "    for match in matches:\n",
    "        food = foods.iloc[food_entity_count]\n",
    "        lab = food_df.loc[idx_lst[food_entity_count]]['label']\n",
    "        food_entity_count -= 1\n",
    "        # replace the pattern, but then find the match of the food entity we just inserted\n",
    "        sentence = sentence.replace(match, food, 1)\n",
    "        match_span = re.search(food, sentence).span()\n",
    "\n",
    "        # use that match to find the index positions of the food entity in the sentence, append\n",
    "        entities.append((match_span[0], match_span[1], lab))\n",
    "    # append the sentence and the position of the entities to the correct dictionary and array\n",
    "    get_food_data(len(matches)).append((sentence, {\"entities\": entities}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I made a bowl of kimbap for my breakfast'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 one_food sentences: (\"There's steamed fish head for breakfast in the bowl this morning\", {'entities': [(8, 25, 'CHINESE')]})\n",
      "29 two_foods sentences: ('I ate jiyu fried shallots and deep fried sausage', {'entities': [(6, 25, 'CHINESE'), (30, 48, 'WESTERN')]})\n",
      "37 three_foods sentences: ('I had a kung pao chicken, a cured ham  cowpeas and five colours pearls at home', {'entities': [(8, 24, 'CHINESE'), (28, 46, 'CHINESE'), (51, 70, 'CHINESE')]})\n",
      "41 one_food items: ('I just finished my ganghoe', {'entities': [(19, 26, 'KOREAN')]})\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ZEPHNG~1\\AppData\\Local\\Temp/ipykernel_6100/363979354.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTEST_FOOD_DATA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} {} items: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_FOOD_DATA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTEST_FOOD_DATA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# READ!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# print the number of food sentences, as well as an example sentence\n",
    "for key in TRAIN_FOOD_DATA:\n",
    "    print(\"{} {} sentences: {}\".format(len(TRAIN_FOOD_DATA[key]), key, TRAIN_FOOD_DATA[key][0]))\n",
    "\n",
    "for key in TEST_FOOD_DATA:\n",
    "    print(\"{} {} items: {}\".format(len(TEST_FOOD_DATA[key]), key, TEST_FOOD_DATA[key][0]))\n",
    "\n",
    "# READ!\n",
    "# can ignore the error below, some lists are empty because we have not enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'one_food': [('I just finished my ganghoe',\n",
       "   {'entities': [(19, 26, 'KOREAN')]}),\n",
       "  (\"I've already eaten a tuna\", {'entities': [(21, 25, 'JAPANESE')]}),\n",
       "  ('I just ate a dangmyeon', {'entities': [(13, 22, 'KOREAN')]}),\n",
       "  (\"I made bagel for yesterday's dinner\", {'entities': [(7, 12, 'WESTERN')]}),\n",
       "  ('When I was having lunch I ate a tonkatsu',\n",
       "   {'entities': [(32, 40, 'JAPANESE')]}),\n",
       "  ('I made a soegogi last night', {'entities': [(9, 16, 'KOREAN')]}),\n",
       "  ('I made a bowl of boksunga for my breakfast',\n",
       "   {'entities': [(17, 25, 'KOREAN')]}),\n",
       "  ('I decided to enjoy some daktoritang', {'entities': [(24, 35, 'KOREAN')]}),\n",
       "  (\"I had oiseon yesterday and I'd like to eat it anyway\",\n",
       "   {'entities': [(6, 12, 'KOREAN')]}),\n",
       "  (\"I had calas yesterday and I'd like to eat it anyway\",\n",
       "   {'entities': [(6, 11, 'WESTERN')]}),\n",
       "  ('When I was having lunch I ate a truffle',\n",
       "   {'entities': [(32, 39, 'WESTERN')]}),\n",
       "  (\"I'm eating a subak\", {'entities': [(13, 18, 'KOREAN')]}),\n",
       "  (\"I made chueotang for yesterday's dinner\",\n",
       "   {'entities': [(7, 16, 'KOREAN')]}),\n",
       "  (\"I'm eating a makchang\", {'entities': [(13, 21, 'KOREAN')]}),\n",
       "  ('I brought a basket of katsudon for breakfast this morning',\n",
       "   {'entities': [(22, 30, 'JAPANESE')]}),\n",
       "  ('I brought a basket of shuizhu for breakfast this morning',\n",
       "   {'entities': [(22, 29, 'CHINESE')]}),\n",
       "  (\"I've already eaten a seolleongtang\", {'entities': [(21, 34, 'KOREAN')]}),\n",
       "  ('When I was having lunch I ate a manchurian',\n",
       "   {'entities': [(32, 42, 'CHINESE')]}),\n",
       "  (\"I've decided to have some yukgaejang for dessert\",\n",
       "   {'entities': [(26, 36, 'KOREAN')]}),\n",
       "  ('When I was having lunch I ate a teppanyaki',\n",
       "   {'entities': [(32, 42, 'JAPANESE')]}),\n",
       "  ('I just ate a kake', {'entities': [(13, 17, 'JAPANESE')]}),\n",
       "  (\"I'm done eating a painaepeul\", {'entities': [(18, 28, 'KOREAN')]}),\n",
       "  (\"There's galbi for breakfast in the bowl this morning\",\n",
       "   {'entities': [(8, 13, 'KOREAN')]}),\n",
       "  (\"I've already eaten a agujjim\", {'entities': [(21, 28, 'KOREAN')]}),\n",
       "  ('I made a bowl of yakiniku for my breakfast',\n",
       "   {'entities': [(17, 25, 'JAPANESE')]}),\n",
       "  (\"There's unagi for breakfast in the bowl this morning\",\n",
       "   {'entities': [(8, 13, 'JAPANESE')]}),\n",
       "  ('This morning, I made a bowl of oritang',\n",
       "   {'entities': [(31, 38, 'KOREAN')]}),\n",
       "  ('I made a hallabong last night', {'entities': [(9, 18, 'KOREAN')]}),\n",
       "  ('I ate my udon', {'entities': [(9, 13, 'JAPANESE')]}),\n",
       "  ('I had a bowl of budae', {'entities': [(16, 21, 'KOREAN')]}),\n",
       "  (\"I had sinseollo yesterday and I'd like to eat it anyway\",\n",
       "   {'entities': [(6, 15, 'KOREAN')]}),\n",
       "  ('I only ate the formaggi', {'entities': [(15, 23, 'WESTERN')]}),\n",
       "  ('I decided to enjoy some soondae', {'entities': [(24, 31, 'KOREAN')]}),\n",
       "  ('When I was having lunch I ate a hotpot',\n",
       "   {'entities': [(32, 38, 'JAPANESE')]}),\n",
       "  ('I ate a couple of cheese last night', {'entities': [(18, 24, 'WESTERN')]}),\n",
       "  ('I had japanese with my husband last night',\n",
       "   {'entities': [(6, 14, 'JAPANESE')]}),\n",
       "  ('I brought you some dalgogi on my birthday',\n",
       "   {'entities': [(19, 26, 'KOREAN')]}),\n",
       "  (\"I'm done eating a hwanggi\", {'entities': [(18, 25, 'KOREAN')]}),\n",
       "  (\"I had misua yesterday and I'd like to eat it anyway\",\n",
       "   {'entities': [(6, 11, 'CHINESE')]}),\n",
       "  (\"I've decided to have some furai for dessert\",\n",
       "   {'entities': [(26, 31, 'JAPANESE')]}),\n",
       "  ('I made a bowl of kimbap for my breakfast',\n",
       "   {'entities': [(17, 23, 'KOREAN')]})],\n",
       " 'two_foods': [],\n",
       " 'three_foods': []}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(TEST_FOOD_DATA['one_food']))\n",
    "TEST_FOOD_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_FOOD_DATA['one_food'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.ner.EntityRecognizer at 0x162e385f0>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp=spacy.blank(\"en\")\n",
    "\n",
    "# nlp.create_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1176a117d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would like a place selling sushi at changi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>show me places selling chinese food around town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>show me places selling ribs around town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>show me places selling sushi at town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Article\n",
       "0     I would like a place selling sushi at changi\n",
       "1  show me places selling chinese food around town\n",
       "2          show me places selling ribs around town\n",
       "3             show me places selling sushi at town"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the revision data (just used a random article dataset from a different course I had taken)\n",
    "npr_df = pd.read_csv(\"npr.csv\")\n",
    "\n",
    "# print row and column information\n",
    "npr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_texts = []\n",
    "\n",
    "# convert the articles to spacy objects to better identify the sentences. Disabled unneeded components. # takes ~ 4 minutes\n",
    "for doc in nlp.pipe(npr_df[\"Article\"][:100], batch_size=30, disable=[\"tagger\", \"ner\"]):\n",
    "    for sentence in doc.sents:\n",
    "        revision_texts.append(\" \".join(re.split(\"\\s+\", sentence.text, flags=re.UNICODE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions = []\n",
    "\n",
    "# Use the existing spaCy model to predict the entities, then append them to revision\n",
    "for doc in nlp.pipe(revision_texts, disable=[\"tagger\", \"parser\"]):\n",
    "    \n",
    "    # don't append sentences that have no entities\n",
    "    if len(doc.ents) > 0:\n",
    "        revisions.append((doc.text, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in doc.ents]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show me places selling chinese food around town\n",
      "{'entities': [(23, 30, 'NORP')]}\n"
     ]
    }
   ],
   "source": [
    "# print an example of the revision sentence\n",
    "print(revisions[0][0])\n",
    "\n",
    "# print an example of the revision data\n",
    "print(revisions[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays to store the revision data\n",
    "TRAIN_REVISION_DATA = []\n",
    "TEST_REVISION_DATA = []\n",
    "\n",
    "# create dictionaries to keep count of the different entities\n",
    "TRAIN_ENTITY_COUNTER = {}\n",
    "TEST_ENTITY_COUNTER = {}\n",
    "\n",
    "# This will help distribute the entities (i.e. we don't want 1000 PERSON entities, but only 80 ORG entities)\n",
    "REVISION_SENTENCE_SOFT_LIMIT = 100\n",
    "\n",
    "# helper function for incrementing the revision counters\n",
    "def increment_revision_counters(entity_counter, entities):\n",
    "    for entity in entities:\n",
    "        label = entity[2]\n",
    "        if label in entity_counter:\n",
    "            entity_counter[label] += 1\n",
    "        else:\n",
    "            entity_counter[label] = 1\n",
    "\n",
    "random.shuffle(revisions)\n",
    "for revision in revisions:\n",
    "    # get the entities from the revision sentence\n",
    "    entities = revision[1][\"entities\"]\n",
    "\n",
    "    # simple hack to make sure spaCy entities don't get too one-sided\n",
    "    should_append_to_train_counter = 0\n",
    "    for _, _, label in entities:\n",
    "        if label in TRAIN_ENTITY_COUNTER and TRAIN_ENTITY_COUNTER[label] > REVISION_SENTENCE_SOFT_LIMIT:\n",
    "            should_append_to_train_counter -= 1\n",
    "        else:\n",
    "            should_append_to_train_counter += 1\n",
    "\n",
    "    # simple switch for deciding whether to append to train data or test data\n",
    "    if should_append_to_train_counter >= 0:\n",
    "        TRAIN_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TRAIN_ENTITY_COUNTER, entities)\n",
    "    else:\n",
    "        TEST_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TEST_ENTITY_COUNTER, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOOD 146\n",
      "REVISION 1\n",
      "COMBINED 147\n"
     ]
    }
   ],
   "source": [
    "# combine the food training data\n",
    "TRAIN_FOOD_DATA_COMBINED = TRAIN_FOOD_DATA[\"one_food\"] + TRAIN_FOOD_DATA[\"two_foods\"] + TRAIN_FOOD_DATA[\"three_foods\"]\n",
    "\n",
    "# print the length of the food training data\n",
    "print(\"FOOD\", len(TRAIN_FOOD_DATA_COMBINED))\n",
    "\n",
    "# print the length of the revision training data\n",
    "print(\"REVISION\", len(TRAIN_REVISION_DATA))\n",
    "\n",
    "# join and print the combined length\n",
    "TRAIN_DATA = TRAIN_REVISION_DATA + TRAIN_FOOD_DATA_COMBINED\n",
    "# TRAIN_DATA = TRAIN_FOOD_DATA_COMBINED\n",
    "print(\"COMBINED\", len(TRAIN_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"There's steamed fish head for breakfast in the bowl this morning\",\n",
       "  {'entities': [(8, 25, 'CHINESE')]}),\n",
       " (\"I made dongpo braised pork for yesterday's dinner\",\n",
       "  {'entities': [(7, 26, 'CHINESE')]}),\n",
       " ('I made a crispy fish skin last night', {'entities': [(9, 25, 'CHINESE')]}),\n",
       " ('I just finished my stirfried duck blood',\n",
       "  {'entities': [(19, 39, 'CHINESE')]}),\n",
       " ('I just finished my fried eel slices', {'entities': [(19, 35, 'CHINESE')]}),\n",
       " (\"I've already eaten a rice and gravy\", {'entities': [(21, 35, 'WESTERN')]}),\n",
       " ('I just ate a twice cooked pork', {'entities': [(13, 30, 'CHINESE')]}),\n",
       " ('I ate a couple of xiao long bao last night',\n",
       "  {'entities': [(18, 31, 'CHINESE')]}),\n",
       " ('This morning, I made a bowl of stuffed fish balls',\n",
       "  {'entities': [(31, 49, 'CHINESE')]}),\n",
       " ('I decided to enjoy some chili con carne',\n",
       "  {'entities': [(24, 39, 'WESTERN')]}),\n",
       " (\"I'm done eating a five colours shrimp\",\n",
       "  {'entities': [(18, 37, 'CHINESE')]}),\n",
       " ('I ate my mac and cheese', {'entities': [(9, 23, 'WESTERN')]}),\n",
       " ('I brought you some braised sliced pork on my birthday',\n",
       "  {'entities': [(19, 38, 'CHINESE')]}),\n",
       " ('I had a bowl of maxwell street polish',\n",
       "  {'entities': [(16, 37, 'WESTERN')]}),\n",
       " (\"I made spring chicken chestnuts for yesterday's dinner\",\n",
       "  {'entities': [(7, 31, 'CHINESE')]}),\n",
       " ('I only ate the braised spare ribs', {'entities': [(15, 33, 'CHINESE')]}),\n",
       " ('I decided to enjoy some fried stuffed beancurd',\n",
       "  {'entities': [(24, 46, 'CHINESE')]}),\n",
       " (\"I've decided to have some roasted turkey for dessert\",\n",
       "  {'entities': [(26, 40, 'WESTERN')]}),\n",
       " (\"I'm done eating a shrimp creole\", {'entities': [(18, 31, 'WESTERN')]}),\n",
       " ('I had some saengseon jjigae at dinner last night',\n",
       "  {'entities': [(11, 27, 'KOREAN')]}),\n",
       " ('I had some carrot cake at dinner last night',\n",
       "  {'entities': [(11, 22, 'CHINESE')]}),\n",
       " (\"I had rice cakes yesterday and I'd like to eat it anyway\",\n",
       "  {'entities': [(6, 16, 'KOREAN')]}),\n",
       " ('I just finished my dolsot bibimbap', {'entities': [(19, 34, 'KOREAN')]}),\n",
       " ('I brought a basket of miyeok guk for breakfast this morning',\n",
       "  {'entities': [(22, 32, 'KOREAN')]}),\n",
       " ('I decided to have some fuqi feipian as a little bonus',\n",
       "  {'entities': [(23, 35, 'CHINESE')]}),\n",
       " ('I had some saengseon gui at dinner last night',\n",
       "  {'entities': [(11, 24, 'KOREAN')]}),\n",
       " (\"There's hot dog for breakfast in the bowl this morning\",\n",
       "  {'entities': [(8, 15, 'WESTERN')]}),\n",
       " ('I had kimchi pancake with my husband last night',\n",
       "  {'entities': [(6, 20, 'KOREAN')]}),\n",
       " ('I had some glorified rice at dinner last night',\n",
       "  {'entities': [(11, 25, 'WESTERN')]}),\n",
       " (\"I had chicken bog yesterday and I'd like to eat it anyway\",\n",
       "  {'entities': [(6, 17, 'WESTERN')]}),\n",
       " (\"I'm eating a makchang gui\", {'entities': [(13, 25, 'KOREAN')]}),\n",
       " (\"I had braised frog yesterday and I'd like to eat it anyway\",\n",
       "  {'entities': [(6, 18, 'CHINESE')]}),\n",
       " ('This morning, I made a bowl of guo tie',\n",
       "  {'entities': [(31, 38, 'CHINESE')]}),\n",
       " ('I had gyeran jjim with my husband last night',\n",
       "  {'entities': [(6, 17, 'KOREAN')]}),\n",
       " (\"I'm eating a sanchae bibimbap\", {'entities': [(13, 29, 'KOREAN')]}),\n",
       " (\"There's steamed pork for breakfast in the bowl this morning\",\n",
       "  {'entities': [(8, 20, 'CHINESE')]}),\n",
       " ('I ate a couple of chongqing spicy last night',\n",
       "  {'entities': [(18, 33, 'CHINESE')]}),\n",
       " ('I decided to enjoy some mala chicken', {'entities': [(24, 36, 'CHINESE')]}),\n",
       " ('I just ate a budae jjigae', {'entities': [(13, 25, 'KOREAN')]}),\n",
       " ('I just finished my oyster omelette', {'entities': [(19, 34, 'CHINESE')]}),\n",
       " ('I brought a basket of spanish rice for breakfast this morning',\n",
       "  {'entities': [(22, 34, 'WESTERN')]}),\n",
       " (\"I'm eating a roasted beef\", {'entities': [(13, 25, 'WESTERN')]}),\n",
       " ('I brought a basket of ban mian for breakfast this morning',\n",
       "  {'entities': [(22, 30, 'CHINESE')]}),\n",
       " (\"I'm done eating a kung pao\", {'entities': [(18, 26, 'CHINESE')]}),\n",
       " (\"There's whitebait omelet for breakfast in the bowl this morning\",\n",
       "  {'entities': [(8, 24, 'CHINESE')]}),\n",
       " ('I brought you some ngo hiang on my birthday',\n",
       "  {'entities': [(19, 28, 'CHINESE')]}),\n",
       " ('I decided to enjoy some kimchi bokkeumbap',\n",
       "  {'entities': [(24, 41, 'KOREAN')]}),\n",
       " ('I only ate the bean curd', {'entities': [(15, 24, 'CHINESE')]}),\n",
       " ('I made a podo last night', {'entities': [(9, 13, 'KOREAN')]}),\n",
       " (\"There's hoe for breakfast in the bowl this morning\",\n",
       "  {'entities': [(8, 11, 'KOREAN')]}),\n",
       " ('I brought you some sesame on my birthday',\n",
       "  {'entities': [(19, 25, 'CHINESE')]}),\n",
       " ('I had sandwich with my husband last night',\n",
       "  {'entities': [(6, 14, 'WESTERN')]}),\n",
       " ('I ate a couple of chijeu last night', {'entities': [(18, 24, 'KOREAN')]}),\n",
       " ('This morning, I made a bowl of promodoro',\n",
       "  {'entities': [(31, 40, 'WESTERN')]}),\n",
       " ('I decided to enjoy some douchi', {'entities': [(24, 30, 'CHINESE')]}),\n",
       " (\"I'm eating a goetta\", {'entities': [(13, 19, 'WESTERN')]}),\n",
       " (\"I'm eating a nurungji\", {'entities': [(13, 21, 'KOREAN')]}),\n",
       " ('I made a raw last night', {'entities': [(9, 12, 'JAPANESE')]}),\n",
       " ('I just ate a western', {'entities': [(13, 20, 'WESTERN')]}),\n",
       " ('I ate a couple of hotate last night', {'entities': [(18, 24, 'JAPANESE')]}),\n",
       " ('I just ate a saeujeot', {'entities': [(13, 21, 'KOREAN')]}),\n",
       " ('I decided to have some kurobuta as a little bonus',\n",
       "  {'entities': [(23, 31, 'JAPANESE')]}),\n",
       " (\"I'm done eating a ddalgi\", {'entities': [(18, 24, 'KOREAN')]}),\n",
       " ('I ate my naengmyeon', {'entities': [(9, 19, 'KOREAN')]}),\n",
       " ('Last night, I ordered some gumbo', {'entities': [(27, 32, 'WESTERN')]}),\n",
       " ('This morning, I made a bowl of risotto',\n",
       "  {'entities': [(31, 38, 'WESTERN')]}),\n",
       " (\"I made natto for yesterday's dinner\", {'entities': [(7, 12, 'JAPANESE')]}),\n",
       " (\"There's creamy for breakfast in the bowl this morning\",\n",
       "  {'entities': [(8, 14, 'WESTERN')]}),\n",
       " ('I ate a couple of oisaengchae last night',\n",
       "  {'entities': [(18, 29, 'KOREAN')]}),\n",
       " ('I decided to have some teishoku as a little bonus',\n",
       "  {'entities': [(23, 31, 'JAPANESE')]}),\n",
       " ('I decided to enjoy some jamong', {'entities': [(24, 30, 'KOREAN')]}),\n",
       " ('I just ate a vinegar', {'entities': [(13, 20, 'CHINESE')]}),\n",
       " ('I brought you some yukhoe on my birthday',\n",
       "  {'entities': [(19, 25, 'KOREAN')]}),\n",
       " ('I ate my umi', {'entities': [(9, 12, 'JAPANESE')]}),\n",
       " ('I ate my hickory', {'entities': [(9, 16, 'WESTERN')]}),\n",
       " ('I just finished my dubujeon', {'entities': [(19, 27, 'KOREAN')]}),\n",
       " ('I ate my popiah', {'entities': [(9, 15, 'CHINESE')]}),\n",
       " ('Last night, I ordered some haejangguk', {'entities': [(27, 37, 'KOREAN')]}),\n",
       " ('I just finished my hobakjuk', {'entities': [(19, 27, 'KOREAN')]}),\n",
       " (\"I'm eating a bolognese\", {'entities': [(13, 22, 'WESTERN')]}),\n",
       " ('I ate jiyu fried shallots and deep fried sausage',\n",
       "  {'entities': [(6, 25, 'CHINESE'), (30, 48, 'WESTERN')]}),\n",
       " ('I had a bowl of clams in soup with charleston red rice and I wanted to go to the mall today',\n",
       "  {'entities': [(16, 29, 'CHINESE'), (35, 54, 'WESTERN')]}),\n",
       " ('I ate fried shrimps and sundubu jjigae',\n",
       "  {'entities': [(6, 19, 'CHINESE'), (24, 38, 'KOREAN')]}),\n",
       " ('I ate a cheonggukjang jjigae and a gopchang jeongol for lunch',\n",
       "  {'entities': [(8, 28, 'KOREAN'), (35, 51, 'KOREAN')]}),\n",
       " ('I had a buffalo wings and a lebanon bologna today',\n",
       "  {'entities': [(8, 21, 'WESTERN'), (28, 43, 'WESTERN')]}),\n",
       " ('I made a dak galbi and sichuan hotpot for lunch',\n",
       "  {'entities': [(9, 18, 'KOREAN'), (23, 37, 'CHINESE')]}),\n",
       " ('I ate live octopus and spare ribs',\n",
       "  {'entities': [(6, 18, 'KOREAN'), (23, 33, 'CHINESE')]}),\n",
       " ('I ate a tex mex with polish boy in the morning',\n",
       "  {'entities': [(8, 15, 'WESTERN'), (21, 31, 'WESTERN')]}),\n",
       " ('I ate a sauteed shrimps and a purple rice for lunch',\n",
       "  {'entities': [(8, 23, 'CHINESE'), (30, 41, 'WESTERN')]}),\n",
       " ('I ate a kongbiji jjigae with andong jjimdak in the morning',\n",
       "  {'entities': [(8, 23, 'KOREAN'), (29, 43, 'KOREAN')]}),\n",
       " ('I ate a jeonbokjjim and a chawanmushi for lunch',\n",
       "  {'entities': [(8, 19, 'KOREAN'), (26, 37, 'JAPANESE')]}),\n",
       " ('last night, a oden was sent to me with jeongol',\n",
       "  {'entities': [(14, 18, 'JAPANESE'), (39, 46, 'KOREAN')]}),\n",
       " ('I ate a pizza and a gamjatang for lunch',\n",
       "  {'entities': [(8, 13, 'WESTERN'), (20, 29, 'KOREAN')]}),\n",
       " ('I ate a sundubu and a namul for lunch',\n",
       "  {'entities': [(8, 15, 'KOREAN'), (22, 27, 'KOREAN')]}),\n",
       " ('I had a bowl of kbbq with karage and I wanted to go to the mall today',\n",
       "  {'entities': [(16, 20, 'KOREAN'), (26, 32, 'JAPANESE')]}),\n",
       " ('I ate aburi and turkey',\n",
       "  {'entities': [(6, 11, 'JAPANESE'), (16, 22, 'WESTERN')]}),\n",
       " ('today I ate a linguine and a gyeran for lunch',\n",
       "  {'entities': [(14, 22, 'WESTERN'), (29, 35, 'KOREAN')]}),\n",
       " ('I made a bbolsal and patbap for lunch',\n",
       "  {'entities': [(9, 16, 'KOREAN'), (21, 27, 'KOREAN')]}),\n",
       " ('today I ate a jambalaya and a marinara for lunch',\n",
       "  {'entities': [(14, 23, 'WESTERN'), (30, 38, 'WESTERN')]}),\n",
       " ('I ate tang and tempura',\n",
       "  {'entities': [(6, 10, 'KOREAN'), (15, 22, 'JAPANESE')]}),\n",
       " ('I made a matcha and samgyetang for lunch',\n",
       "  {'entities': [(9, 15, 'JAPANESE'), (20, 30, 'KOREAN')]}),\n",
       " ('today I ate a sushi and a tteokguk for lunch',\n",
       "  {'entities': [(14, 19, 'JAPANESE'), (26, 34, 'KOREAN')]}),\n",
       " ('I made a galbijjim and miso for lunch',\n",
       "  {'entities': [(9, 18, 'KOREAN'), (23, 27, 'JAPANESE')]}),\n",
       " ('today I ate a boribap and a salad for lunch',\n",
       "  {'entities': [(14, 21, 'KOREAN'), (28, 33, 'WESTERN')]}),\n",
       " ('I ate cheesy and jjigae',\n",
       "  {'entities': [(6, 12, 'WESTERN'), (17, 23, 'KOREAN')]}),\n",
       " ('I made a japchae and korean for lunch',\n",
       "  {'entities': [(9, 16, 'KOREAN'), (21, 27, 'KOREAN')]}),\n",
       " ('I ate a gobchang with rosti in the morning',\n",
       "  {'entities': [(8, 16, 'KOREAN'), (22, 27, 'WESTERN')]}),\n",
       " ('today I ate a samgyeopsal and a beikeon for lunch',\n",
       "  {'entities': [(14, 25, 'KOREAN'), (32, 39, 'KOREAN')]}),\n",
       " ('I had a galbitang and a manduguk today',\n",
       "  {'entities': [(8, 17, 'KOREAN'), (24, 32, 'KOREAN')]}),\n",
       " ('I had a kung pao chicken, a cured ham  cowpeas and five colours pearls at home',\n",
       "  {'entities': [(8, 24, 'CHINESE'),\n",
       "    (28, 46, 'CHINESE'),\n",
       "    (51, 70, 'CHINESE')]}),\n",
       " ('I tasted some italian hot dog, fried gluten balls and old duck stewed at the office',\n",
       "  {'entities': [(14, 29, 'WESTERN'),\n",
       "    (31, 49, 'CHINESE'),\n",
       "    (54, 69, 'CHINESE')]}),\n",
       " ('I had a stir fried meat, a sauteed broad beans and spring bamboo shoots at home',\n",
       "  {'entities': [(8, 23, 'CHINESE'),\n",
       "    (27, 46, 'CHINESE'),\n",
       "    (51, 71, 'CHINESE')]}),\n",
       " (\"I've already had a bag of breakfast sausage, fried rice and fried chicken from the fridge\",\n",
       "  {'entities': [(26, 43, 'WESTERN'),\n",
       "    (45, 55, 'CHINESE'),\n",
       "    (60, 73, 'KOREAN')]}),\n",
       " ('I had a kimchi jjigae, a saeujeot jjigae and doraji saengchae at home',\n",
       "  {'entities': [(8, 21, 'KOREAN'), (25, 40, 'KOREAN'), (45, 61, 'KOREAN')]}),\n",
       " ('I ate a fragrant snails with seokhwa gui and gim gui just now',\n",
       "  {'entities': [(8, 23, 'CHINESE'), (29, 40, 'KOREAN'), (45, 52, 'KOREAN')]}),\n",
       " ('I devoured a dirty rice, bamboo roo and lotus seeds',\n",
       "  {'entities': [(13, 23, 'WESTERN'),\n",
       "    (25, 35, 'CHINESE'),\n",
       "    (40, 51, 'CHINESE')]}),\n",
       " ('I tasted some aglio olio, mashed potato and longjing tea at the office',\n",
       "  {'entities': [(14, 24, 'WESTERN'),\n",
       "    (26, 39, 'WESTERN'),\n",
       "    (44, 56, 'CHINESE')]}),\n",
       " (\"There's a basket of doenjang jjigae, dandan noodles and steamed egg that I consumed\",\n",
       "  {'entities': [(20, 35, 'KOREAN'),\n",
       "    (37, 51, 'CHINESE'),\n",
       "    (56, 67, 'CHINESE')]}),\n",
       " ('I had a beer duck, a gochujang jjigae and spaghetti donut at home',\n",
       "  {'entities': [(8, 17, 'CHINESE'), (21, 37, 'KOREAN'), (42, 57, 'WESTERN')]}),\n",
       " ('I had a drunken ribs, a deodeok gui and bologna sausage at home',\n",
       "  {'entities': [(8, 20, 'CHINESE'), (24, 35, 'KOREAN'), (40, 55, 'WESTERN')]}),\n",
       " ('Last night, I ate an raw beef with gobchang gui and chili peppers',\n",
       "  {'entities': [(21, 29, 'KOREAN'), (35, 47, 'KOREAN'), (52, 65, 'CHINESE')]}),\n",
       " ('Last night, I ate an beoseot gui with hawaiian haystack and mapo tofu',\n",
       "  {'entities': [(21, 32, 'KOREAN'),\n",
       "    (38, 55, 'WESTERN'),\n",
       "    (60, 69, 'CHINESE')]}),\n",
       " ('I took a jadu, doenjang and maeuntang on the weekend',\n",
       "  {'entities': [(9, 13, 'KOREAN'), (15, 23, 'KOREAN'), (28, 37, 'KOREAN')]}),\n",
       " ('I devoured a tomahawk, fishball and okonomiyaki',\n",
       "  {'entities': [(13, 21, 'WESTERN'),\n",
       "    (23, 31, 'CHINESE'),\n",
       "    (36, 47, 'JAPANESE')]}),\n",
       " ('I had a takoyaki, a mentaiko and burger at home',\n",
       "  {'entities': [(8, 16, 'JAPANESE'),\n",
       "    (20, 28, 'JAPANESE'),\n",
       "    (33, 39, 'WESTERN')]}),\n",
       " ('I devoured a kongnamul, gyul and cheonggukjang',\n",
       "  {'entities': [(13, 22, 'KOREAN'), (24, 28, 'KOREAN'), (33, 46, 'KOREAN')]}),\n",
       " (\"I've already had a bag of yakitori, dwaejigogi and seuteikeu from the fridge\",\n",
       "  {'entities': [(26, 34, 'JAPANESE'),\n",
       "    (36, 46, 'KOREAN'),\n",
       "    (51, 60, 'KOREAN')]}),\n",
       " ('I had a lasagna, a dakbokkeumtan and lamian at home',\n",
       "  {'entities': [(8, 15, 'WESTERN'), (19, 32, 'KOREAN'), (37, 43, 'CHINESE')]}),\n",
       " ('I took a sukchae, kongnamulbap and mulgogi on the weekend',\n",
       "  {'entities': [(9, 16, 'KOREAN'), (18, 30, 'KOREAN'), (35, 42, 'KOREAN')]}),\n",
       " ('I had a chinese, a bulgogi and potstickers at home',\n",
       "  {'entities': [(8, 15, 'CHINESE'), (19, 26, 'KOREAN'), (31, 42, 'CHINESE')]}),\n",
       " (\"There's a basket of kongbiji, seongnyu and jjamppong that I consumed\",\n",
       "  {'entities': [(20, 28, 'KOREAN'), (30, 38, 'KOREAN'), (43, 52, 'KOREAN')]}),\n",
       " ('I had a bibimbap, a soba and gam at home',\n",
       "  {'entities': [(8, 16, 'KOREAN'), (20, 24, 'JAPANESE'), (29, 32, 'KOREAN')]}),\n",
       " ('I took a dorayaki, sannakji and yanpi on the weekend',\n",
       "  {'entities': [(9, 17, 'JAPANESE'),\n",
       "    (19, 27, 'KOREAN'),\n",
       "    (32, 37, 'CHINESE')]}),\n",
       " ('I ate a hobakjeon with fries and tenderloin just now',\n",
       "  {'entities': [(8, 17, 'KOREAN'), (23, 28, 'WESTERN'), (33, 43, 'WESTERN')]}),\n",
       " ('I took a ogokbap, croissant and ebi on the weekend',\n",
       "  {'entities': [(9, 16, 'KOREAN'),\n",
       "    (18, 27, 'WESTERN'),\n",
       "    (32, 35, 'JAPANESE')]}),\n",
       " ('I devoured a bossam, minshengguo and fish',\n",
       "  {'entities': [(13, 19, 'KOREAN'),\n",
       "    (21, 32, 'CHINESE'),\n",
       "    (37, 41, 'JAPANESE')]}),\n",
       " ('I ate a shabu with poke and kimchi just now',\n",
       "  {'entities': [(8, 13, 'JAPANESE'),\n",
       "    (19, 23, 'JAPANESE'),\n",
       "    (28, 34, 'KOREAN')]}),\n",
       " (\"There's a basket of sanddalgi, zhanjiangmian and saengchae that I consumed\",\n",
       "  {'entities': [(20, 29, 'KOREAN'), (31, 44, 'CHINESE'), (49, 58, 'KOREAN')]}),\n",
       " ('I ate a manggo with onigiri and guk just now',\n",
       "  {'entities': [(8, 14, 'KOREAN'), (20, 27, 'JAPANESE'), (32, 35, 'KOREAN')]}),\n",
       " ('Last night, I ate an sirloin with dango and sukhoe',\n",
       "  {'entities': [(21, 28, 'WESTERN'),\n",
       "    (34, 39, 'JAPANESE'),\n",
       "    (44, 50, 'KOREAN')]}),\n",
       " ('I had a orenji, a spaghetti and ribs at home',\n",
       "  {'entities': [(8, 14, 'KOREAN'), (18, 27, 'WESTERN'), (32, 36, 'WESTERN')]}),\n",
       " ('I had a origogi, a dubu and mac at home',\n",
       "  {'entities': [(8, 15, 'KOREAN'), (19, 23, 'KOREAN'), (28, 31, 'WESTERN')]}),\n",
       " ('I ate a bamboo with chips and ramen just now',\n",
       "  {'entities': [(8, 14, 'CHINESE'),\n",
       "    (20, 25, 'WESTERN'),\n",
       "    (30, 35, 'JAPANESE')]}),\n",
       " ('I ate a ddukbokki with samyetang and sagwa just now',\n",
       "  {'entities': [(8, 17, 'KOREAN'), (23, 32, 'KOREAN'), (37, 42, 'KOREAN')]}),\n",
       " ('I tasted some ggakdugi, chaudin and sukiyaki at the office',\n",
       "  {'entities': [(14, 22, 'KOREAN'),\n",
       "    (24, 31, 'WESTERN'),\n",
       "    (36, 44, 'JAPANESE')]}),\n",
       " ('I took a dandanmian, saengseon and yakisoba on the weekend',\n",
       "  {'entities': [(9, 19, 'CHINESE'),\n",
       "    (21, 30, 'KOREAN'),\n",
       "    (35, 43, 'JAPANESE')]})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_FOOD_DATA_COMBINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add NER to the pipeline and the new label\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"FOOD\")\n",
    "\n",
    "# get the names of the components we want to disable during training\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.vocab.Vocab at 0x11758ba1c10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import Example\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses (1/30) {'ner': 0.0}\n",
      "Losses (2/30) {'ner': 0.0}\n",
      "Losses (3/30) {'ner': 0.0}\n",
      "Losses (4/30) {'ner': 0.0}\n",
      "Losses (5/30) {'ner': 0.0}\n",
      "Losses (6/30) {'ner': 0.0}\n",
      "Losses (7/30) {'ner': 0.0}\n",
      "Losses (8/30) {'ner': 0.0}\n",
      "Losses (9/30) {'ner': 0.0}\n",
      "Losses (10/30) {'ner': 0.0}\n",
      "Losses (11/30) {'ner': 0.0}\n",
      "Losses (12/30) {'ner': 0.0}\n",
      "Losses (13/30) {'ner': 0.0}\n",
      "Losses (14/30) {'ner': 0.0}\n",
      "Losses (15/30) {'ner': 0.0}\n",
      "Losses (16/30) {'ner': 0.0}\n",
      "Losses (17/30) {'ner': 0.0}\n",
      "Losses (18/30) {'ner': 0.0}\n",
      "Losses (19/30) {'ner': 0.0}\n",
      "Losses (20/30) {'ner': 0.0}\n",
      "Losses (21/30) {'ner': 0.0}\n",
      "Losses (22/30) {'ner': 0.0}\n",
      "Losses (23/30) {'ner': 0.0}\n",
      "Losses (24/30) {'ner': 0.0}\n",
      "Losses (25/30) {'ner': 0.0}\n",
      "Losses (26/30) {'ner': 0.0}\n",
      "Losses (27/30) {'ner': 0.0}\n",
      "Losses (28/30) {'ner': 0.0}\n",
      "Losses (29/30) {'ner': 0.0}\n",
      "Losses (30/30) {'ner': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# start the training loop, only training NER\n",
    "epochs = 30\n",
    "optimizer = nlp.resume_training()\n",
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    \n",
    "    # batch up the examples using spaCy's minibatc\n",
    "    for epoch in range(epochs):\n",
    "        examples = TRAIN_DATA\n",
    "        random.shuffle(examples)\n",
    "        batches = minibatch(examples, size=sizes)\n",
    "        losses = {}\n",
    "        \n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            doc = nlp.make_doc(texts[0])\n",
    "            a = annotations[0]\n",
    "            token_ref = []\n",
    "            tags_ref = []\n",
    "            words = []\n",
    "            for ent1 in a['entities']:\n",
    "                start = ent1[0]\n",
    "                end = ent1[1]\n",
    "                label =texts[0][start:end]\n",
    "                token = ent1[2]\n",
    "                token_ref.append(token)\n",
    "                tags_ref.append(label)\n",
    "                words.append(token)\n",
    "            predicted = Doc(nlp.vocab, words)\n",
    "            example = Example.from_dict(predicted, {\"words\": token_ref, \"tags\": tags_ref})\n",
    "            nlp.update([example], sgd=optimizer, drop=0.1, losses=losses)\n",
    "\n",
    "        print(\"Losses ({}/{})\".format(epoch + 1, epochs), losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    sushi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is delicious</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I had a fish and chips for lunch \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    today\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I decided to have \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    kimchi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " as a little treat for myself.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I ordered \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    basmati rice\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", leaf spinach and cheese from \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    yesterday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display sentence involving original entities\n",
    "spacy.displacy.render(nlp(\"sushi is delicious\"), style=\"ent\")\n",
    "\n",
    "# display sentences involving target entity\n",
    "spacy.displacy.render(nlp(\"I had a fish and chips for lunch today.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"I decided to have kimchi as a little treat for myself.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"I ordered basmati rice, leaf spinach and cheese from Tesco yesterday\"), style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I was eating a burger\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'show me places selling chinese food around town'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [(23, 30, 'NORP')]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 30, 'NORP')\n",
      "word picked out is: chinese\n"
     ]
    }
   ],
   "source": [
    "a = annotations[0]\n",
    "for ent1 in a['entities']:\n",
    "    print(ent1)\n",
    "    start = ent1[0]\n",
    "    end = ent1[1]\n",
    "    print('word picked out is:', texts[0][start:end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f30e6e673d75f527aa5ee319b4d022f5eb4fb185f07f95f5c9360e29c610252a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('hacknroll': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
